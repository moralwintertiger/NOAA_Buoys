{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook goes through the scraping of geostationary satellite (GOES-16) data from UWI RealEarth\n",
    "\n",
    "RealEarth API documentation: https://realearth.ssec.wisc.edu/doc/api.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast #allows to deal with string as dictionary\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First build a list of available dataset timestamps for a given channel\n",
    "In this case we are looking at the channel G16-ABI-CONUS-BAND02, which collects cloud data from a high-res full-continental US image collected by the GOES-east satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify available datasets:\n",
    "\n",
    "def available_datasets_by_time(channel):\n",
    "    abi_band2_list = []\n",
    "    base_url = \"https://realearth.ssec.wisc.edu/api/times?products=\"\n",
    "    #url = requests.get(base_url + str(channel))\n",
    "    url = base_url + channel\n",
    "    r = requests.get(url)\n",
    "    list_text = ast.literal_eval(r.text)\n",
    "    for val in list_text.values():\n",
    "        if val not in abi_band2_list:\n",
    "            abi_band2_list.append(val)\n",
    "    return abi_band2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, subset this list to only contain the first timestamp for each hour\n",
    "\n",
    "The reasoning behind this is that the current dataset of weather values from DarkskyAPI is hourly and we are looking for one image per weather instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for unique combinations between date and hour stamps and use these to build a new list:\n",
    "\n",
    "def unique_time_stamps(input_list):\n",
    "    output_list = []\n",
    "    for i in range(len(input_list[0])):\n",
    "        date, hour = input_list[0][i][0:8], input_list[0][i][-6:-4]\n",
    "        if date+hour not in output_list:\n",
    "            output_list.append(date+hour)\n",
    "            #output_list.append(date[0:4] + \"-\" + date[4:6] + \"-\" + date[6:8] + \"+\" + hour)\n",
    "            \n",
    "    return output_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat times:\n",
    "\n",
    "def reformat_times(input_list):\n",
    "    '''\n",
    "    takes as input a time_stamp in format: 2018080521, as from unique_time_stamps()\n",
    "    '''\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        output_list.append(i[0:4] + \"-\" + i[4:6] + \"-\" + i[6:8] + \"+\" + i[8:10])\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_times_raw(input_list):\n",
    "    '''\n",
    "    takes as input a time_stamp in format: 20180805.213725, as from unique_time_stamps()\n",
    "    '''\n",
    "    output_list = []\n",
    "    for i in input_list[0]:\n",
    "        output_list.append(i[0:4] + \"-\" + i[4:6] + \"-\" + i[6:8] + \"+\" + i[9:11] + \":\" + i[11:13])\n",
    "        \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of urls:\n",
    "\n",
    "def url_list(list_of_times):\n",
    "    \n",
    "    '''\n",
    "    collects urls using realearth api. simply replaces timestamp using predefined list of times.\n",
    "    '''\n",
    "    \n",
    "    url_output_list = []\n",
    "    base_url = \"https://realearth.ssec.wisc.edu/api/image?products=G16-ABI-CONUS-BAND02&time=\"\n",
    "    end_url = \"&center=37.77,-122.41&zoom=9&width=150&height=150\"\n",
    "    for i in list_of_times:\n",
    "        url_output_list.append(base_url + i + end_url)\n",
    "    return url_output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull images for each url:\n",
    "\n",
    "Images are pulled as pngs, then saved as 1D arrays (which can be used as input for ML and re-viewed via reshaping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take input url, retrieve png, and flatten to 1D array (C-style):\n",
    "\n",
    "def flatten_png_from_url(input_url):\n",
    "    response = requests.get(input_url)\n",
    "    if response.status_code == 200:        \n",
    "        input_png = Image.open(BytesIO(response.content))  \n",
    "        as_array = np.array(input_png)[:,:,0]\n",
    "        flat_array = as_array.flatten()\n",
    "        return flat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build images dataframe with urls and timestamps\n",
    "\n",
    "images_df = pd.DataFrame(columns=[\"url\", \"img_array\"])\n",
    "images_df[\"url\"] = [url_list(time) for time in image_times]\n",
    "images_df[\"img_array\"] = [flatten_png_from_url(url) for url in input_urls]\n",
    "images_df[\"time_stamps\"] = [time for time in unique_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save an example image:\n",
    "\n",
    "url = \"https://realearth.ssec.wisc.edu/api/image?products=G16-ABI-CONUS-BAND02&time=2018-08-05+18&center=37.77,-122.41&zoom=9&width=300&height=300\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "image\n",
    "image.save(\"realearth20180805SF.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resulting dataframe. This will be updated periodically to build dataset.\n",
    "\n",
    "Ideally, the above functions should be built into a pipeline that opens the images_df, appends new data, then re-saves. This should be integrated with the DarkSkyAPI scripts to build a single df with arrays, timestamps, and temp values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stockton_may_data_converted.to_csv(\"stockton_may_temp_humidity.csv\", encoding='utf-8', index=True)\n",
    "\n",
    "images_df.to_csv(\"RealEarthImages080818.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipelines for data to allow for easy updating with new data:\n",
    "\n",
    "Rather than feeding data from one function to another sequentially, a function should be built to manage those handoffs and to open an existing dataframe and append data to it. Below are functions to build new dataframes for a given channel or to update an existing dataframe. Some functionality needs to be added such that dataframes are updated and old files are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find new data to append:\n",
    "\n",
    "def update_existing_df(existing_df, channel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes existing df and channel as inputs and returns a list of urls for datasets not yet in the existing df.\n",
    "    To be used to append new data to an existing df.\n",
    "    \"\"\"\n",
    "    #read in old df and initialize a list for time points that do not occur in the old df:\n",
    "    new_timepoints = []\n",
    "    df = pd.read_csv(existing_df)\n",
    "    all_datasets = available_datasets_by_time(channel)\n",
    "    datasets_by_hour = unique_time_stamps(all_datasets)\n",
    "    \n",
    "    #set list of time points from old df:\n",
    "    cross_check = list(df[\"time_stamps\"])\n",
    "    \n",
    "    #only append time points from new df that were not in old df:\n",
    "    for item in datasets_by_hour:\n",
    "        if int(item) not in cross_check:\n",
    "            new_timepoints.append(item)\n",
    "    image_times = reformat_times(new_timepoints)\n",
    "    \n",
    "    #collect urls to scrape for new df:\n",
    "    input_urls = url_list(image_times)\n",
    "    \n",
    "    #build new df:\n",
    "    new_df = pd.DataFrame(columns=[\"url\", \"img_array\"])\n",
    "    new_df[\"url\"] = [url_list(time) for time in image_times]\n",
    "    new_df[\"img_array\"] = [flatten_png_from_url(url) for url in input_urls]\n",
    "    new_df[\"time_stamps\"] = [time for time in image_times]\n",
    "    \n",
    "    #merge new and old dfs and reset index:\n",
    "    merged_df = pd.concat([df, new_df]).reset_index(drop=True)\n",
    "    \n",
    "    #save as csv with current date and time such that most recent file can be referenced on next build:\n",
    "    \n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    merged_df.to_csv(\"output_csv_files/\" + timestr + \"_\" + channel + \"_Merge.csv\", encoding='utf-8', index=False)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = build_new_df(\"RealEarthImages080818.csv\", \"G16-ABI-CONUS-BAND02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180809-115217_G16-ABI-CONUS-BAND02_Merge.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd output_csv_files\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_new_df(channel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Builds a new df from a given channel. Note that this builds with all time points, as opposed to only hourly.\n",
    "    \"\"\"\n",
    "    #identify and parse available datasets, then format times and retrieve their URLs:\n",
    "    all_datasets = available_datasets_by_time(channel)\n",
    "    image_times = reformat_times_raw(all_datasets)\n",
    "    input_urls = url_list(image_times)\n",
    "    \n",
    "    #build df:\n",
    "    new_df = pd.DataFrame(columns=[\"time_stamps\", \"img_array\"])\n",
    "    new_df[\"img_array\"] = [flatten_png_from_url(url) for url in input_urls]\n",
    "    new_df[\"time_stamps\"] = [time for time in image_times]\n",
    "    \n",
    "    #remove rows where img_array is empty:\n",
    "    new_df = new_df[new_df[\"img_array\"].map(lambda d: np.sum(d)) != 0]\n",
    "\n",
    "    #\n",
    "    \n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    new_df.to_csv(\"output_csv_files/\" + channel + timestr + \"_All_Datapoints.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "    return new_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build new df with all timepoints:\n",
    "\n",
    "all_channel2 = build_new_df(\"G16-ABI-CONUS-BAND02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/Projects/Currents/Curr-env/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>array_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.146000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.321707e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.100406e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.679200e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.118782e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.708774e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.296976e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.153728e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          array_sum\n",
       "count  1.146000e+03\n",
       "mean   1.321707e+06\n",
       "std    1.100406e+06\n",
       "min    5.679200e+04\n",
       "25%    1.118782e+05\n",
       "50%    1.708774e+06\n",
       "75%    2.296976e+06\n",
       "max    3.153728e+06"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect data:\n",
    "\n",
    "all_channel_2_copy.shape\n",
    "all_channel_2_copy[\"array_sum\"] = [np.sum(all_channel_2_copy.iloc[i][\"img_array\"]) for i in range(len(all_channel_2_copy))]\n",
    "\n",
    "\n",
    "all_channel_2_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot total karl coverage by time with bokeh:\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import LinearAxis, Range1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(all_channel_2_copy))\n",
    "t = all_channel_2_copy[\"array_sum\"]\n",
    "bfig = figure(tools=\"pan,box_zoom,reset,save\", title=\"Total Karl by Time, 080418 to 080818\")\n",
    "bfig.line(x, t, line_width = 3, line_color = \"red\")\n",
    "bfig.xaxis.axis_label = \"Time Point\"\n",
    "bfig.yaxis.axis_label = \"Total Karl\"\n",
    "show(bfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(all_channel_2_copy))\n",
    "\n",
    "t = all_channel_2_copy[\"array_sum\"]\n",
    "bfig = figure(tools=\"pan,box_zoom,reset,save\", title=\"Total Karl by Time, 080418 to 080818\")\n",
    "bfig.vbar(x=x, top=t, width = 1, line_color = \"purple\")\n",
    "bfig.xaxis.axis_label = \"Time Point\"\n",
    "bfig.yaxis.axis_label = \"Total Karl\"\n",
    "show(bfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.util.hex import hexbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/Projects/Currents/Curr-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_channel_2_copy[\"hour\"] = [int(all_channel_2_copy.iloc[i].hour) for i in range(len(all_channel_2_copy))]\n",
    "type(all_channel_2_copy.iloc[0].hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_channel_2_copy.hour\n",
    "t = all_channel_2_copy[\"array_sum\"]\n",
    "bfig = figure(tools=\"pan,box_zoom,reset,save\", title=\"total Karl by time\")\n",
    "bfig.circle(x, t, size=10, alpha=0.5, color=\"purple\")\n",
    "bfig.xaxis.axis_label = \"UTC Hour\"\n",
    "bfig.yaxis.axis_label = \"Total Karl\"\n",
    "show(bfig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
