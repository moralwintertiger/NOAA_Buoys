{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook goes through the scraping of geostationary satellite (GOES-16) data from UWI RealEarth\n",
    "\n",
    "RealEarth API documentation: https://realearth.ssec.wisc.edu/doc/api.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast #allows to deal with string as dictionary\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First build a list of available dataset timestamps for a given channel\n",
    "In this case we are looking at the channel G16-ABI-CONUS-BAND02, which collects cloud data from a high-res full-continental US image collected by the GOES-east satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify available datasets:\n",
    "\n",
    "def available_datasets_by_time(channel):\n",
    "    abi_band2_list = []\n",
    "    base_url = \"https://realearth.ssec.wisc.edu/api/times?products=\"\n",
    "    #url = requests.get(base_url + str(channel))\n",
    "    url = base_url + channel\n",
    "    r = requests.get(url)\n",
    "    list_text = ast.literal_eval(r.text)\n",
    "    for val in list_text.values():\n",
    "        if val not in abi_band2_list:\n",
    "            abi_band2_list.append(val)\n",
    "    return abi_band2_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, subset this list to only contain the first timestamp for each hour\n",
    "\n",
    "The reasoning behind this is that the current dataset of weather values from DarkskyAPI is hourly and we are looking for one image per weather instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for unique combinations between date and hour stamps and use these to build a new list:\n",
    "\n",
    "def unique_time_stamps(input_list):\n",
    "    output_list = []\n",
    "    for i in range(len(input_list[0])):\n",
    "        date, hour = input_list[0][i][0:8], input_list[0][i][-6:-4]\n",
    "        if date+hour not in output_list:\n",
    "            output_list.append(date+hour)\n",
    "            #output_list.append(date[0:4] + \"-\" + date[4:6] + \"-\" + date[6:8] + \"+\" + hour)\n",
    "            \n",
    "    return output_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat times:\n",
    "\n",
    "def reformat_times(input_list):\n",
    "    output_list = []\n",
    "    for i in input_list:\n",
    "        output_list.append(i[0:4] + \"-\" + i[4:6] + \"-\" + i[6:8] + \"+\" + i[8:10])\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate list of urls:\n",
    "\n",
    "def url_list(list_of_times):\n",
    "    url_output_list = []\n",
    "    base_url = \"https://realearth.ssec.wisc.edu/api/image?products=G16-ABI-CONUS-BAND02&time=\"\n",
    "    end_url = \"&center=37.77,-122.41&zoom=9&width=150&height=150\"\n",
    "    for i in list_of_times:\n",
    "        url_output_list.append(base_url + i + end_url)\n",
    "    return url_output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull images for each url:\n",
    "\n",
    "Images are pulled as pngs, then saved as 1D arrays (which can be used as input for ML and re-viewed via reshaping).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take input url, retrieve png, and flatten to 1D array (C-style):\n",
    "\n",
    "def flatten_png_from_url(input_url):\n",
    "    response = requests.get(input_url)\n",
    "    if response.status_code == 200:        \n",
    "        input_png = Image.open(BytesIO(response.content))  \n",
    "        as_array = np.array(input_png)[:,:,0]\n",
    "        flat_array = as_array.flatten()\n",
    "        return flat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build images dataframe with urls and timestamps\n",
    "\n",
    "images_df = pd.DataFrame(columns=[\"url\", \"img_array\"])\n",
    "images_df[\"url\"] = [url_list(time) for time in image_times]\n",
    "images_df[\"img_array\"] = [flatten_png_from_url(url) for url in input_urls]\n",
    "images_df[\"time_stamps\"] = [time for time in unique_times]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save an example image:\n",
    "\n",
    "url = \"https://realearth.ssec.wisc.edu/api/image?products=G16-ABI-CONUS-BAND02&time=2018-08-05+18&center=37.77,-122.41&zoom=9&width=300&height=300\"\n",
    "response = requests.get(url)\n",
    "image = Image.open(BytesIO(response.content))\n",
    "image\n",
    "image.save(\"realearth20180805SF.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save resulting dataframe. This will be updated periodically to build dataset.\n",
    "\n",
    "Ideally, the above functions should be built into a pipeline that opens the images_df, appends new data, then re-saves. This should be integrated with the DarkSkyAPI scripts to build a single df with arrays, timestamps, and temp values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stockton_may_data_converted.to_csv(\"stockton_may_temp_humidity.csv\", encoding='utf-8', index=True)\n",
    "\n",
    "images_df.to_csv(\"RealEarthImages080818.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline for data to allow for easy updating with new data:\n",
    "\n",
    "Rather than feeding data from one function to another sequentially, a function should be built to manage those handoffs and to open an existing dataframe and append data to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find new data to append:\n",
    "\n",
    "def build_new_df(input_df, channel):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes existing df and channel as inputs and returns a list of urls for datasets not yet in the existing df.\n",
    "    To be used to append new data to an existing df.\n",
    "    \"\"\"\n",
    "    #read in old df and initialize a list for time points that do not occur in the old df:\n",
    "    new_timepoints = []\n",
    "    df = pd.read_csv(input_df)\n",
    "    all_datasets = available_datasets_by_time(channel)\n",
    "    datasets_by_hour = unique_time_stamps(all_datasets)\n",
    "    \n",
    "    #set list of time points from old df:\n",
    "    cross_check = list(df[\"time_stamps\"])\n",
    "    \n",
    "    #only append time points from new df that were not in old df:\n",
    "    for item in datasets_by_hour:\n",
    "        if int(item) not in cross_check:\n",
    "            new_timepoints.append(item)\n",
    "    image_times = reformat_times(new_timepoints)\n",
    "    \n",
    "    #collect urls to scrape for new df:\n",
    "    input_urls = url_list(image_times)\n",
    "    \n",
    "    #build new df:\n",
    "    new_df = pd.DataFrame(columns=[\"url\", \"img_array\"])\n",
    "    new_df[\"url\"] = [url_list(time) for time in image_times]\n",
    "    new_df[\"img_array\"] = [flatten_png_from_url(url) for url in input_urls]\n",
    "    new_df[\"time_stamps\"] = [time for time in image_times]\n",
    "    \n",
    "    #merge new and old dfs and reset index:\n",
    "    merged_df = pd.concat([df, new_df]).reset_index(drop=True)\n",
    "    \n",
    "    #save as csv with current date and time such that most recent file can be referenced on next build:\n",
    "    \n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    merged_df.to_csv(\"output_csv_files/\" + timestr + \"_\" + channel + \"_Merge.csv\", encoding='utf-8', index=False)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = build_new_df(\"RealEarthImages080818.csv\", \"G16-ABI-CONUS-BAND02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180809-115217_G16-ABI-CONUS-BAND02_Merge.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd output_csv_files\n",
    "ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
